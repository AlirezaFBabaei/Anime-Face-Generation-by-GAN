{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A6aBbmPY1Stc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers as L\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import cv2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hzAvNWC316sV"
      },
      "outputs": [],
      "source": [
        "IMG_H = 64\n",
        "IMG_W = 64\n",
        "IMG_C = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "nxg4Wowo2jlw"
      },
      "outputs": [],
      "source": [
        "def create_dir(path):\n",
        "  if not os.path.exists(path):\n",
        "    os.makedirs(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "imOKNdHh2r8w"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "  img = tf.io.read_file(image_path)\n",
        "  img = tf.io.decode_jpeg(img, channels=3)\n",
        "  img = tf.image.resize(img, [64, 64])\n",
        "  img = tf.cast(img, tf.float32)\n",
        "  img = (img - 127.5) / 127.5\n",
        "\n",
        "  return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ae8zcBw03GES"
      },
      "outputs": [],
      "source": [
        "def tf_dataset(images_path, batch_size):\n",
        "  ds = tf.data.Dataset.from_tensor_slices(images_path)\n",
        "  ds = ds.shuffle(buffer_size=1000).map(load_image, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
        "  ds = ds.batch(batch_size).prefetch(buffer_size = tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cjlTR4jj3htE"
      },
      "outputs": [],
      "source": [
        "def build_generator(latent_dim):\n",
        "    noise = L.Input((latent_dim), name=\"noise_input\")\n",
        "\n",
        "    ## 1.\n",
        "    x = L.Dense(256)(noise)\n",
        "    x = L.LeakyReLU(0.2)(x)\n",
        "\n",
        "    ## 2.\n",
        "    x = L.Dense(1024)(x)\n",
        "    x = L.LeakyReLU(0.2)(x)\n",
        "\n",
        "    ## 2.\n",
        "    x = L.Dense(4096)(x)\n",
        "    x = L.LeakyReLU(0.2)(x)\n",
        "\n",
        "    ## 4.\n",
        "    x = L.Dense(IMG_H * IMG_W * IMG_C)(x)\n",
        "    x = L.LeakyReLU(0.2)(x)\n",
        "\n",
        "    ## 5.\n",
        "    x = L.Reshape((IMG_H, IMG_W, IMG_C))(x)\n",
        "\n",
        "    fake_output = L.Activation(\"tanh\")(x)\n",
        "\n",
        "    return Model(noise, fake_output, name=\"generator\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xjz2gtNV5WGP"
      },
      "outputs": [],
      "source": [
        "def build_discriminator():\n",
        "    inputs = L.Input((IMG_H, IMG_W, IMG_C), name=\"disc_input\")\n",
        "\n",
        "    ## 1.\n",
        "    x = L.Flatten()(inputs)\n",
        "\n",
        "    ## 2.\n",
        "    x = L.Dense(4096)(x)\n",
        "    x = L.LeakyReLU(0.2)(x)\n",
        "    x = L.Dropout(0.3)(x)\n",
        "\n",
        "    ## 3.\n",
        "    x = L.Dense(1024)(x)\n",
        "    x = L.LeakyReLU(0.2)(x)\n",
        "    x = L.Dropout(0.3)(x)\n",
        "\n",
        "    ## 4.\n",
        "    x = L.Dense(256)(x)\n",
        "    x = L.LeakyReLU(0.2)(x)\n",
        "    x = L.Dropout(0.3)(x)\n",
        "\n",
        "    ## 5.\n",
        "    x = L.Dense(1)(x)\n",
        "\n",
        "    return Model(inputs, x, name=\"discriminator\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eF6JtbYF6zu9"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(real_images, latent_dim, generator, discriminator, g_opt, d_opt):\n",
        "  batch_size = tf.cast(tf.shape(real_images)[0], tf.float32)\n",
        "  bce_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1)\n",
        "\n",
        "  ## Discriminator\n",
        "  batch_size = tf.cast(batch_size, tf.int32)\n",
        "  latent_dim = tf.cast(latent_dim, tf.int32)\n",
        "  noise = tf.random.normal([batch_size, latent_dim])\n",
        "\n",
        "  for _ in range(2):\n",
        "    with tf.GradientTape() as dtape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(real_images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      d_real_loss = bce_loss(tf.ones_like(real_output), real_output)\n",
        "      d_fake_loss = bce_loss(tf.zeros_like(fake_output), fake_output)\n",
        "      d_loss = d_real_loss + d_fake_loss\n",
        "\n",
        "      d_grad = dtape.gradient(d_loss, discriminator.trainable_variables)\n",
        "      d_opt.apply_gradients(zip(d_grad, discriminator.trainable_variables))\n",
        "\n",
        "\n",
        "      ## Generator\n",
        "      with tf.GradientTape() as gtape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        g_loss = bce_loss(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "        g_grad = gtape.gradient(g_loss, generator.trainable_variables)\n",
        "        g_opt.apply_gradients(zip(g_grad, generator.trainable_variables))\n",
        "\n",
        "        return d_loss, g_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PlE6vudBFRcJ"
      },
      "outputs": [],
      "source": [
        "def save_plot(examples, epoch, n):\n",
        "  n = int(n)\n",
        "  examples = (examples + 1) / 2.0\n",
        "  examples = examples * 255\n",
        "  file_name = f\"samples/generated_plot_epoch-{epoch+1}.png\"\n",
        "\n",
        "  cat_image = None\n",
        "\n",
        "  for i in range(n):\n",
        "    start_idx = i*n\n",
        "    end_idx = (i+1) * n\n",
        "    image_list = examples[start_idx:end_idx]\n",
        "\n",
        "    if i == 0:\n",
        "      cat_image = np.concatenate(image_list, axis=1)\n",
        "\n",
        "    else:\n",
        "      tmp = np.concatenate(image_list, axis=1)\n",
        "      cat_image = np.concatenate([cat_image, tmp], axis=0)\n",
        "\n",
        "  cv2.imwrite(file_name, cat_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WahCO-aYGmjF"
      },
      "source": [
        "Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oGy357kLGXT-"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "latent_dim = 64\n",
        "num_epochs = 700\n",
        "n_samples = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "br1WuDmfGjjI",
        "outputId": "6d97db19-7988-4108-b655-17c103ccf02c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Images: 19097\n"
          ]
        }
      ],
      "source": [
        "images_path = glob('images/*.jpg')\n",
        "print(f\"Images: {len(images_path)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fDsrY0WGsydT"
      },
      "outputs": [],
      "source": [
        "create_dir(\"Samples\")\n",
        "create_dir(\"Saved model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_h30ovoetL-M"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "QfLw2ARWtBHi"
      },
      "outputs": [],
      "source": [
        "g_model = build_generator(latent_dim)\n",
        "d_model = build_discriminator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K04-gtOLtYET",
        "outputId": "4fceb951-4059-4096-989d-6dd4feaa13d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " noise_input (InputLayer)    [(None, 64)]              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               16640     \n",
            "                                                                 \n",
            " leaky_re_lu (LeakyReLU)     (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1024)              263168    \n",
            "                                                                 \n",
            " leaky_re_lu_1 (LeakyReLU)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 4096)              4198400   \n",
            "                                                                 \n",
            " leaky_re_lu_2 (LeakyReLU)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 12288)             50343936  \n",
            "                                                                 \n",
            " leaky_re_lu_3 (LeakyReLU)   (None, 12288)             0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 64, 64, 3)         0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 64, 64, 3)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,822,144\n",
            "Trainable params: 54,822,144\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "g_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59T_uNYjtZm2",
        "outputId": "e61376d9-7b9a-4fbc-ae8e-ca5e9d7e8b77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " disc_input (InputLayer)     [(None, 64, 64, 3)]       0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12288)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4096)              50335744  \n",
            "                                                                 \n",
            " leaky_re_lu_4 (LeakyReLU)   (None, 4096)              0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 4096)              0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1024)              4195328   \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 1024)              0         \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1024)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 256)               262400    \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 256)               0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 54,793,729\n",
            "Trainable params: 54,793,729\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "d_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "G2X0lGXHthff"
      },
      "outputs": [],
      "source": [
        "d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9L0P07hBdGRP"
      },
      "outputs": [],
      "source": [
        "images_dataset = tf_dataset(images_path, batch_size)\n",
        "seed = np.random.normal(size=(n_samples, latent_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<PrefetchDataset element_spec=TensorSpec(shape=(None, 64, 64, 3), dtype=tf.float32, name=None)>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "images_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3I71Dc3GdJOp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[1/700] 6.71s - d_loss: 1.0993 - g_loss: 0.7549\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[2/700] 4.73s - d_loss: 1.1207 - g_loss: 0.8228\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "[3/700] 4.73s - d_loss: 1.1795 - g_loss: 0.8890\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "        start = time.time()\n",
        "\n",
        "        d_loss = 0.0\n",
        "        g_loss = 0.0\n",
        "\n",
        "        for image_batch in images_dataset:\n",
        "            d_batch_loss, g_batch_loss = train_step(image_batch, latent_dim, g_model, d_model, g_optimizer, d_optimizer)\n",
        "            d_loss += d_batch_loss\n",
        "            g_loss += g_batch_loss\n",
        "\n",
        "        d_loss = d_loss/len(images_dataset)\n",
        "        g_loss = g_loss/len(images_dataset)\n",
        "\n",
        "        g_model.save(\"saved_model/g_model.h5\")\n",
        "        d_model.save(\"saved_model/d_model.h5\")\n",
        "\n",
        "        examples = g_model.predict(seed, verbose=0)\n",
        "        save_plot(examples, epoch, np.sqrt(n_samples))\n",
        "\n",
        "        time_taken = time.time() - start\n",
        "        print(f\"[{epoch+1:1.0f}/{num_epochs}] {time_taken:2.2f}s - d_loss: {d_loss:1.4f} - g_loss: {g_loss:1.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C5UwyDhdMSf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
